# SMART-VOICE-ASSISTANT-FOR-THE-BLIND-USING-ARTIFICIAL-INTELLIGENCE
A system that uses artificial intelligence, image recognition, and machine learning to assist visually impaired individuals in interacting with their surroundings. The system would convert visual data into an alternative modality, such as a voice assistant, to aid independent living for the visually impaired

## ðŸ‘¨â€ðŸ’» Team Members
- Praveen Varshney - 20MID0102  
- Riya Pandita - 20MID0108  
- Vishwanath Sinha - 20MID0118  

## ðŸ“˜ Course Info
- Course Code: CSI3003  
- Title: Artificial Intelligence and Expert Systems  
- Semester: Winter 2022-2023  
- Instructor: Prof. Annapurna Jonnalagadda  
- Department: School of Computer Science and Engineering (SCOPE)

---

## ðŸ§© Problem Statement
Create a system using AI and image recognition to assist the blind by converting visual data into audio, enabling independent navigation and daily living for those with vision impairment.

## ðŸ’¡ Motivation
To improve autonomy and self-reliance for the visually impaired and reduce their dependence on others through AI-powered solutions.

## ðŸŒ Applications
- **Navigation** with real-time direction guidance  
- **Reading** text and documents aloud  
- **Smart Home Control** via voice commands  
- **Personal Assistant** functions like reminders and calls  

## ðŸ§  Literature Survey Highlights
- Google Glass-based visual scene analysis for BVIP  
- Neural image captioning using attention mechanisms  
- Sequence-to-sequence approaches for describing images  

## âš™ï¸ Architecture
- Image features extracted using VGG16  
- Descriptions tokenized and modeled with LSTM  
- Voice output generated for both images and video frames  

## ðŸ“ Implementation Overview
1. Mount Google Drive and set up Kaggle  
2. Load and modify VGG16 for feature extraction  
3. Tokenize and clean image captions  
4. Train model using image-caption pairs  
5. Generate real-time voice output for inputs  

## ðŸ“ˆ Result Highlights
- Output for videos like:  
  > "Two puppies are playing in the grass"  
- Evaluation based on time taken for pre-processing and caption generation

## ðŸš€ Future Enhancements
- Direct video stream processing without frame extraction  
- Training for more epochs to improve caption quality  
- Improve accuracy with enhanced datasets and model tuning  

## ðŸ“‚ Code and Resources
[Google Colab Notebook](https://colab.research.google.com/drive/11CjQPhBBnAzepRiPLWzEq5nakTbHCIR6?usp=sharing)

## ðŸ“š References
Refer to the full list of 14 academic and web sources in the accompanying PDF document.

